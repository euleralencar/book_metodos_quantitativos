# Aula 04

### Introdução

Neste capítulo buscamos examinar possíveis relações entre duas ou mais variáveis aleatórias. As noções de probabilidade condicional e independência descritas nos capítulos anteriores serão requeridas novamente em um contexto multivariado.

Neste capítulo apresentaremos os seguintes temas:

-   Apresentar a distribuição de probabilidades conjuntas para expressar a relação entre dois ou mais variáveis.
-   Encontrar distribuições de probabilidades marginais que mostrem o comportamento de uma única variável sem o efeito das outras.
-   Apresentar a distribuição de probabilidades condicionais que expressam o efeito de um subconjunto de variáveis sobre outro subconjunto de variáveis.
-   Definir e verificar as implicações da independência entre variáveis aleatórias.
-   Definir medidas de associação entre duas variáveis, tais como a covariância e correlação.

### Definição - Vetor aleatório

Considere uma experiência aleatória com espaço amostral ($\Omega$), gerando o espaço de probabilidade ($( \Omega,F,P)$). Sejam ($X_1, X_2, X_3, \ldots, X_k$), funções que associam números reais a cada resultado da experiência. Então, ($(X_1, X_2, X_3, \ldots, X_k)$) é um vetor aleatório da dimensão ($k$). Em particular, temos ($k = 2$), ($(X_1, X_2)$) será chamado de vetor aleatório bidimensional ou variável aleatória bidimensional.

### Definição: A Função Distribuição Acumulada

Um vetor aleatório bivariado ($x, y$) é:

$F(x,y) = P(\omega \in \Omega: X(\omega) \leq x, Y(\omega) \leq y)$

$F(x,y) = P(X \leq x \text{ e } Y \leq y) = P(X \leq x , Y \leq y)$

Em geral, para ($k$) variáveis aleatórias:

$(\text{Digite})$

### Definição: Função Probabilidade e função Densidade de Probabilidade

**Discreto:**

$f(x,y) = P(X=x, Y=y) = P(\omega \in \Omega: X(\omega)=x, Y(\omega)=y)$

**Contínuo:**

$f(x,y) = \frac{\partial^2 F(x,y)}{\partial x \partial y}$

Da definição, calculamos ($F$) como:

$F(x_0,y_0) = \int_{-\infty}^{x_0} \int_{-\infty}^{y_0} f(x,y) \, dx \, dy$

### Definição: Função de Probabilidade Conjunta

**Variáveis aleatórias discretas:**

Sejam ($X_1$) e ($X_2$) variáveis aleatórias discretas. A densidade conjunta ($f(x_1,x_2)$) é uma função não-negativa tal que:

$f(x_1,x_2) = P(X_1=x_1, X_2=x_2)$

e satisfaz:

i)  Propriedade 1: ($0 < f(x_1,x_2) < 1$)

ii) Propriedade 2:

$\sum_{\forall x_1} \sum_{\forall x_2} f(x_1,x_2) = \sum_{\forall x_1} \sum_{\forall x_2} P(X_1=x_1, X_2=x_2) = 1$

Essa definição pode ser estendida para ($n$) variáveis ($X_1, X_2, \ldots, X_n$):

$f(x_1,x_2,\ldots,x_n) = P(X_1=x_1, X_2=x_2, \ldots, X_n=x_n)$

$\sum_{\forall x_1} \ldots \sum_{\forall x_n} P(X_1=x_1, X_2=x_2, \ldots, X_n=x_n) = \sum_{\forall x_1} \ldots \sum_{\forall x_n} f(x_1,\ldots,x_n) = 1$

**Variáveis aleatórias contínuas:**

Sejam ($X_1$) e ($X_2$) variáveis aleatórias contínuas. A função densidade conjunta de ($X_1$) e ($X_2$) é uma função ($f(x_1,x_2)$):

$f: \mathbb{R}^2 \rightarrow \mathbb{R}^+$

com as seguintes propriedades:

i)  Propriedade 1: ($f(x_1,x_2) > 0$)

ii) Propriedade 2:

$\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(x_1,x_2) \, dx_2 \, dx_1 = 1$

Para algum ($A \subset \mathbb{R}^2$) podemos definir como:

$P((X_1,X_2) \in A) = \iint_A f(x_1,x_2) \, dx_1 \, dx_2$

Ou também da seguinte forma:

Para calcular a probabilidade de ($X_1$) e ($X_2$) estarem em intervalos de números reais, calculamos a integral:

$P(a \leq X_1 \leq b, c \leq X_2 \leq d) = \int_a^b \int_c^d f(x_1,x_2) \, dx_1 \, dx_2$

OBS: Colocar uma figura mostrando o volume (DIGITAR)

Podemos estender para o caso de ($n$) variáveis ($X_1, X_2, \ldots, X_n$):

$P(a \leq X_1 \leq b, \ldots, c \leq X_n \leq d) = \int_a^b \ldots \int_c^d f(x_1,\ldots,x_n) \, dx_1 \ldots dx_n$

OBS: No caso discreto, a função de distribuição conjunta representa uma probabilidade envolvendo ($X_1$) e ($X_2$), o mesmo não ocorre no caso contínuo. $P(X_1 \leq x_1, \ldots, X_n \leq x_n)$

$F(x_1,\ldots,x_n) = \sum_{\forall x_1} \ldots \sum_{\forall x_n} P(X_1 \leq x_1, \ldots, X_n \leq x_n)$

### Definição: Densidade Condicional

A função probabilidade ou densidade condicional de $Y$ dado $X$ é definida se $f_{\cdot}(x) > 0$ e é dada por:

$f(Y|X) = \frac{f(x,y)}{f(x)}$

Lembre-se da definição de probabilidade condicional para eventos:

$P(A|B) = \frac{P(A \cap B)}{P(B)}$

Observe que:

$f(Y|X) \neq f(X|Y)$

Analogamente, a função probabilidade ou densidade condicional de $X$ dado $Y$ é definida se $f_{\cdot}(Y) > 0$ e é dada por:

$f(X|Y) = \frac{f(x,y)}{f(y)}$

### Exemplo

Coloque verdadeiro e falso:

-   ( $f(Y|X) = f(X|Y)$ )
-   ( $f(Y|X=1) = f(Y|X=2)$ )
-   ( $E((Y|X=1)) = E(Y|X=2)$ )

**Solução:** Todas são falsas.

Esta definição é análoga à definição de probabilidade condicional para eventos.

Com esta definição podemos calcular probabilidades do tipo:

$P(a < Y < b|X=x) = \int_a^b f(Y|X=x) \, dy$

É a probabilidade condicional de que $a < Y < b$ dado que $X = x$.

Nota: não faz diferença os símbolos \<, menor igual no caso contínuo. Para o caso discreto, temos:

$P(Y \leq a|X=x) = \sum_{\forall y} P(Y|X=x) = \sum{\forall y} f(Y|X=x)$

Para uma variável aleatória contínua essa densidade permite calcular probabilidades condicionais:

$P(a < y < b|X=x) = \int_a^b f(Y|x) \, dy$

Ou

$P(a < y < b|X=1) = \int_a^b f(Y|X=1) \, dy$

Usando como informativo a variável $Y$:

$P(a < x < b|Y=1) = \int_a^b f(X|Y=1) \, dx$

### Média condicional e variância condicional

Lembre-se do cálculo da média incondicional:

-   V.a.d → ( $E(X) = \sum_{x=-\infty}^{\infty} x \, P(X=x)$ )
-   V.a.c → ( $E(X) = \int_{-\infty}^{\infty} x \, f(x) \, dx$ )

Por exemplo, desejamos calcular $E(Y|X)$. Como procedemos?

$E(Y|X) = \int y \, f(Y|X) \, dy$

Da definição:

$f(Y|X) = \frac{f(x,y)}{f(x)}$

### Definição (Esperança condicional)

Seja $X$ e $Y$ variáveis aleatórias contínuas, então, a esperança condicional de $Y$ dado $X$, denotado por $E(Y|X)$, é dado por:

$E(Y|X) = \int_{-\infty}^{\infty} y \, f(Y|X) \, dy$

A definição para o caso discreto é análoga.

$E(Y|X) = \sum_{x=-\infty}^{\infty} y \, f(Y|X)$

### Definição: Variância condicional

A variância de distribuição $f(X|Y)$ é chamada de variância condicional a $Y$ dado $X$.

Variância Condicional

$\text{Var}(Y|X) = E[(Y-E(Y|X))^2|X]$

Ou também:

$\text{Var}(Y|X) = E(Y^2 |X) - (E(Y|X))^2$

Para um determinado valor de $X=x$:

$\text{Var}(Y|X=x) = E[(Y-E(Y|X=x))^2|X=x]$

Por exemplo para $x=2$:

$\text{Var}(Y|X=2) = E[(Y-E(Y|X=2))^2|X=2]$

### Definição: Independência

As variáveis aleatórias $X$ e $Y$ são estatisticamente independentes se somente se:

$f(X|Y) = f(X), \forall x \text{ e } \forall y$

Lembre da definição para eventos independentes:

$P(A|B) = P(A)$

Dizemos que o evento $A$ é independente do evento $B$.

Da mesma forma:

$f(Y|X) = f(Y), \forall x \text{ e } \forall y$

Suponha que $Y$ e $X$ são variáveis aleatórias independentes. Da definição de probabilidade condicional:

$f(Y) = f(Y|X) = \frac{f(X,Y)}{f(X)}$

Rearranjando os termos chegamos ao resultado de que quando $X$ e $Y$ são independentes temos igualdade entre a densidade conjunta e o produto das densidades marginais:

$f(X,Y) = f(X)f(Y)$

### Exemplo 1 - Seja $X$ e $Y$ variáveis aleatórias discretas com densidade conjunta dado a seguir:

```{=tex}
\begin{array}{|c|c|c|c|}
\hline
P(Y=y) & X=0 & X=1 & X=2 & X=3 \\
\hline
y=0 & 0,3 & 0 & 0,1 & 0 \\
y=1 & 0,1 & 0,1 & 0,1 & 0,1 \\
y=2 & 0 & 0,1 & 0,1 & 0 \\
\hline
\end{array}
```
Pede-se:

-   Encontre as densidades marginais $f(X)$ e $f(Y)$
-   Encontre $E(X)$ e $\text{Var}(X)$
-   Encontre a densidade condicional de $Y$ dado $X=1$
-   Encontre a densidade condicional de $Y$ dado $X=0$
-   Encontre $E(Y|X=1)$ e a $\text{Var}(Y|X=1)$

**Solução**

a)  Inicialmente, observamos que temos variáveis aleatórias discretas com valores para $X$ de {0, 1, 2, 3} e valores para $Y$ de {0, 1, 2}. Portanto,

$f(x) = \sum_{\forall y} f(x,y)$ ou $f(x) = \sum{\forall y} P(X=x,Y=y)$

$f(x) = \sum_{\forall y} f(x,y) = f(x,0) + f(x,1) + f(x,2) = P(X=x,Y=0) + P(X=x,Y=1) + P(X=x,Y=2)$

```{=tex}
\begin{array}{|c|c|}
\hline
X & f(x)=P(X=x) \\
\hline
0 & 0,4 \\
1 & 0,2 \\
2 & 0,3 \\
3 & 0,1 \\
\hline
\end{array}
```
Falta encontrar para $f(y)$ (DIGITAR)

b)  Para calcular a esperança $E(X)$ então: Como é uma variável aleatória discreta: $E(X) = \sum_{x=-\infty}^{\infty} x P(X=x)$ $E(X) = x_0 P(X=x_0) + x_1 P(X=x_1) + x_2 P(X=x_2) + x_3 P(X=x_3)$ $E(X) = 0(0,4) + 1(0,2) + 2(0,3) + 3(0,1)$ $E(X) = 0,2 + 0,6 + 0,3 = 1,1$

Para calcular a $\text{Var}(X)$ então: $\sigma^2 = \text{Var}(X) = E[(X-μ)^2] = \sum_{i=1}^n (x_i-μ)^2 P(X=x_i) = \sum_{i=1}^n (x_i-μ)^2 f(x_i)$ $\sigma^2 = \text{Var}(X) = (0-1,1)^2 \cdot 0,4 + (1-1,1)^2 \cdot 0,2 + (2-1,1)^2 \cdot 0,3 + (3-1,1)^2 \cdot 0,1 = 1,09$

c)  Encontre a densidade condicional de $Y$ dado $X=1$:

$f(y | x=1) = \frac{f(1,y)}{f(1)}$ $f(y | x=1) = \frac{f(1,0)}{0,2} = \frac{0}{0,2} = 0$ $f(y | x=1) = \frac{f(1,1)}{0,2} = \frac{0,1}{0,2} = 0,5$ $f(y | x=1) = \frac{f(1,2)}{0,2} = \frac{0,1}{0,2} = 0,5$

d)  Encontre a densidade condicional de $Y$ dado $X=0$:

$f(y | x=0) = \frac{f(0,y)}{f_x(0)}$ Como $f_x(0) = 0,4$ então: $f(0 | X=0) = \frac{f(0,0)}{0,4} = \frac{0,3}{0,4} = 0,75$ $f(1 | X=0) = \frac{f(0,1)}{0,4} = \frac{0,1}{0,4} = 0,25$ $f(2 | X=0) = \frac{f(0,2)}{0,4} = \frac{0}{0,4} = 0$

d)  Encontre $E(Y | x=1)$ e a $\text{Var}(Y|x=1)$:

Esperança condicional: $E(Y| x=1) = \sum_{\forall y} y , P(Y=y | x=1)$ $E(Y| x=1) = 0 \cdot 0 + 1 \cdot 0,5 + 2 \cdot 0,5 = 1,5$

Variância condicional: $\sigma^2 = \text{Var}(Y|x=1) = E[(Y-μ)^2] = \sum_{\forall y} (y_i-μ)^2 \cdot P(Y=y_i | x=1)$ $\sigma^2 = \text{Var}(Y|x=1) = \sum_{\forall y} (y_i-μ)^2 \cdot f(y_i | x=1)$ $\text{Var}(Y|x=1) = (0-1,5)^2 \cdot 0 + (1-1,5)^2 \cdot 0,5 + (2-1,5)^2 \cdot 0,5$ $\text{Var}(Y|x=1) = 0,25$

### Solução

O espaço amostral é:

$\Omega = {\overbrace{HHH}^{\text{(x=2,y=2)}}; \underbrace{HHT}*{*\text{(x=2,y=1)}}; \overbrace{HTH}^{\text{(x=1, y=1)}}; \underbrace{HTT}\text{(x=1 ,y=0)}; \overbrace{THH}^{\text{(x=1, y=2)}}; \underbrace{THT}*\text{(x=1 ,y=1)}; \overbrace{TTH}^{\text{(x=0, y=1)}}; \underbrace{TTT}\text{(x=0,y=0)}}$

Valores que tomam as variáveis: - ($X = \{0,1,2\}$) - ($Y = \{0,1,2\}$)

**a) Encontre (**$f(x,y)$)

**Resolução:**

```{=tex}
\begin{array}{|c|c|c|}
\hline
f(x,y) & P (\omega \in \Omega | X(\omega)=x,Y(\omega)=y) & P \\
\hline
(0,0) & P(\{TTT\}) & \frac{1}{8} \\
(0,1) & P(\{TTH\}) & \frac{1}{8} \\
(1,0) & P(\{HTT\}) & \frac{1}{8} \\
(1,1) & P(\{HTH;THT\}) & \frac{2}{8} \\
(1,2) & P(\{THH\}) & \frac{1}{8} \\
(2,1) & P(\{HHT\}) & \frac{1}{8} \\
(2,2) & P(\{HHH\}) & \frac{1}{8} \\
(0,2) & P(\{\emptyset\}) & 0 \\
(2,0) & P(\{\emptyset\}) & 0 \\
\hline
\end{array}
```
**b) Encontre (**$f_x (x)$) e ($f_y (y)$)

$f_x (x) = \sum_{\text{todo } y} f(x,y) = f(x,0) + f(x,1) + f(x,2)$

( \begin{align*}
x=0 & : f_1 (0) = f(0,0) + f(0,1) + f(0,2) = \frac{1}{8} + \frac{1}{8} + 0 = \frac{2}{8} \\
x=1 & : f_1 (1) = f(1,0) + f(1,1) + f(1,2) = \frac{1}{8} + \frac{2}{8} + \frac{1}{8} = \frac{4}{8} \\
x=2 & : f_1 (2) = f(2,0) + f(2,1) + f(2,2) = 0 + \frac{1}{8} + \frac{1}{8} = \frac{2}{8}
  \end{align*} )

Logo,

```{=tex}
\begin{array}{|c|c|}
\hline
X & f(x) = P(X=x) \\
\hline
0 & \frac{2}{8} \\
1 & \frac{4}{8} \\
2 & \frac{2}{8} \\
\hline
\end{array}
```
**c) Encontre ( E(X) ) e (** \text{Var}(X) )

$E(X) = \sum\_{\forall x} f(x,y) = 0f(0) + 1f(1) + 2f(2) = 0 \cdot \frac{2}{8} + 1 \cdot \frac{4}{8} + 2 \cdot \frac{2}{8} = 1$

$\text{Var}(X) = E$(X - \mu)\^2\$ = \sum\_{\forall x} (X - \mu)\^2 f(x) = \left(0 - \frac{8}{8}\right)\^2 \frac{2}{8} + \left(1 - \frac{8}{8}\right)\^2 \frac{4}{8} + \left(2 - \frac{8}{8}\right)\^2 \frac{2}{8}\$

**d) Encontre ( E(Y\|X=1) )**

$f(y\|x) = \frac{f(x,y)}{f(x)}$

$f(y\|x=1) = \frac{f(1,y)}{f(1)}$

\[ \begin{align*}
y=0 & : f(Y|X=1) = f(0|1) = \frac{f(1,0)}{f(1)} = \frac{\frac{1}{8}}{\frac{4}{8}} = \frac{1}{4} \\
y=1 & : f(Y|X=1) = f(1|1) = \frac{f(1,1)}{f(1)} = \frac{\frac{2}{8}}{\frac{4}{8}} = \frac{2}{4} \\
y=2 & : f(Y|X=1) = f(2|1) = \frac{f(1,2)}{f(1)} = \frac{\frac{1}{8}}{\frac{4}{8}} = \frac{1}{4}
\end{align*} \]

$Y \quad f(Y|X=1)$ \|$Y$ \|$f(Y|X=1)$ \| \| ------ \| ------------- \| \| 0 \|$\frac{1}{4}$ \| \| 1 \|$\frac{2}{4}$ \| \| 2 \|$\frac{1}{4}$ \|

$E(Y|X=1)$ $0 \times \frac{1}{4} + 1 \times \frac{2}{4} + 2 \times \frac{1}{4} = 1$

$\text{Var}(Y|X=1)$ $\text{Var}(Y|X=1) = E((Y-E(Y│X=1))^2│X=1)$ $= \sum_{y} (Y-E(Y|X=1))^2 f(Y|X=1)$ $= (0-1)^2 (0,25) + (1-1)^2 (0,5) + (2-1)^2 (0,25)$ $= 1(0,25) + 0(0,5) + 1(0,25) = 0,25 + 0,25 = 0,5$

**Exemplo 3:**

Seja $f(x,y) = \begin{cases} kxy^2 & 0 < x < 1 \text{ e } 0 < y < 1 \\ 0 & \text{caso contrário} \end{cases}$, em que $f(x,y)$ é uma função densidade conjunta. Pede-se: a) Encontre o valor de $k$ b) Encontre $E(X)$ e $\text{Var}(X)$ c) Encontre $f(Y|X)$ d) As variáveis $X$ e $Y$ são independentes? e) Encontre $E(Y|X=0,5)$ e $E(Y|X=0,8)$.

**Solução:**

**a) Encontre o valor de** $k$

Da propriedade de função densidade de probabilidade conjunta temos: $\int_{0}^{1} \int_{0}^{1} kxy^2 \,dx\,dy = 1$ $\int_{0}^{1} \int_{0}^{1} kxy^2 \,dx\,dy = \int_{0}^{1} ky^2 \left( \int_{0}^{1} x \,dx \right) \,dy = k\int_{0}^{1} y^2 \,dy = \frac{k}{2} \int_{0}^{1} y^2 \,dy = \frac{k}{6} = 1$

Temos: $\frac{k}{6} = 1 \quad \Rightarrow \quad k = 6$

**b) Encontre** $E(X)$

$E(X) = \int_{0}^{1} x f_X (x) \,dx$

Precisamos encontrar a densidade marginal de $x$ para podermos calcular a esperança: $f_X (x) = \int_{0}^{1} f(x,y) \,dy$

$f_X (x) = \int_{0}^{1} 6xy^2 \,dy = 6x \left[ \frac{y^3}{3} \right]_{0}^{1} = 2x$

Procedendo com o cálculo da esperança:

$E(X) = \int_{0}^{1} x (2x) \,dx = \int_{0}^{1} 2x^2 \,dx = 2 \left[ \frac{x^3}{3} \right]_{0}^{1} = \frac{2}{3}$

**Encontre** $\text{Var}(X)$

$\text{Var}(X) = E([x-E(X)]^2) = \int_{0}^{1} [x-E(X)]^2 f_X (x) \,dx$ $= \int_{0}^{1} [x-\frac{2}{3}]^2 2x \,dx$ $= \int_{0}^{1} 2x(x^2-\frac{4x}{3}+\frac{4}{9}) \,dx$ $= \int_{0}^{1} 2x^3 - \frac{8x^2}{3} + \frac{8x}{9} \,dx$ $= \left[ \frac{x^4}{2} \right]_{0}^{1} - \left[ \frac{8x^3}{9} \right]_{0}^{1} + \left[ \frac{4x^2}{9} \right]_{0}^{1}$ $= \frac{1}{2} - \frac{8}{9} + \frac{4}{9} = \frac{9}{18} - \frac{16}{18} + \frac{8}{18} = \frac{1}{18}$

**Encontre** $f(Y|X)$

$f(Y│X) = \frac{f(x,y)}{f(x)} = \frac{6xy^2}{2x} = 3y^2$

**As variáveis** $X$ e $Y$ são independentes?

Se $X$ e $Y$ são independentes a densidade conjunta de $X$ e $Y$ é igual ao produto das densidades marginais: $f(x,y) = f_X (x) f_Y (y)$

No item b) encontramos $f_X (x) = 2x$. Precisamos obter $f_Y (y)$: $f_Y (y) = \int_{-\infty}^{+\infty} f(x,y) \,dx$ $f_Y (y) = \int_{0}^{1} 6xy^2 \,dx = 6y^2 \left[ \frac{x^2}{2} \right]_{0}^{1} = 3y^2$

Multiplicando as densidades marginais obtemos a densidade conjunta:

$f_X (x) f_Y (y) = 2x \times 3y^2 = 6xy^2 = f(x,y)$

Portanto $X$ e $Y$ são independentes.

**Encontre** $E(Y|X=0,5)$ e $E(Y|X=0,8)$

$E(Y│X=0,5) = \int_{-\infty}^{+\infty} yf(y|x=0,5) \,dy$

$E(Y│X=0,5) = \int_{0}^{1} yf(y|x=0,5) \,dy = \int_{0}^{1} y3y^2 \,dy = 3\int_{0}^{1} y^3 \,dy = 3\left[ \frac{y^4}{4} \right]_{0}^{1} = \frac{3}{4}$

Como a função densidade condicional $f(y|x)$ não depende de $x$, então: $E(Y│X=0,5) = E(Y│X=0,8) = \frac{3}{4}$

**Explique o motivo pelo qual** $E(Y|X=0,5)$ e $E(Y|X=0,8)$ são iguais.

Como $X$ e $Y$ são independentes, $f(Y|X)$ não depende de valores de $x$ e portanto as duas esperanças condicionadas são iguais.

Algumas relações são satisfeitas sobre a hipótese de independência:

-   $E(Y|X) = E(Y)$
-   $E(X|Y) = E(X)$
-   $E(XY) = E(X)E(Y)$

### Definição: Esperança Matemática

#### Para o caso contínuo:

$E_{XY} (g(X,Y)) = \iint g(X,Y)f(X,Y) \,dxdy$

#### Para o caso discreto:

$E_{XY} (g(X,Y)) = \sum \sum g(X,Y)f(X,Y) \quad \forall x \text{ e } \forall y$

Em particular, para $g(X,Y) = XY$ temos:

$E(XY) = E_{XY} (XY) = \iint XY \,f(X,Y) \,dxdy$

Se $X$ e $Y$ são independentes, então:

$E(XY) = E(X)E(Y)$

Para duas funções contínuas $U$ e $V$:

$E(U(X)V(Y)) = E(U(X)) \cdot E(V(Y))$

**Exemplo:**

Sendo $X$ e $Y$ independentes então:

$E(e^{tX} e^{tY} ) = E(e^{tX} )E(e^{tY} )$

**Teorema: Lei das Expectativas Iteradas (LEI)**

Sejam $X$ e $Y$ duas variáveis aleatórias. Então:

$E(Y) = E(E(Y |X))$

Ou também:

$E(X) = E(E(X |Y))$

Escrito de outra forma:

$E_Y (Y) = E_X (E_{(y|x)} (Y |X))$

Em geral, considerando uma função $g$, a LEI fica como:

$E_{XY} [g(Y)] = E_X (E_{(y |x)} (g(Y) | X))$

**Prova**

Mostraremos $E(X) = E(E(X |Y))$ para o caso contínuo.

Partimos da definição de esperança, densidade marginal e densidade condicional:

$E(X) = \int_{x} x f_X (x) \,dx \quad (1)$

$f_X (x) = \int_{y} f(x,y) \,dy \quad (2)$

$f(x|y) = \frac{f(x,y)}{f_Y (y)} \rightarrow f(x,y) = f(x|y) f_Y (y) \quad (3)$

Substituindo $(2)$ em $(1)$:

$E(X) = \int_{x} x \cdot f_X (x) \,dx = \int_{x} x \cdot \left[ \int_{y} f(x,y) \,dy \right] \,dx \quad (4)$

Substituindo $(3)$ em $(4)$:

$E(X) = \int_{x} \int_{y} x \cdot f(x|y) \cdot f_Y (y) \,dy \,dx$

$E(X) = \int_{y} \left[ \int_{x} x \cdot f(x|y) \,dx \right] \cdot f_Y (y) \,dy$

$E(X) = \int_{y} E(X|Y) \cdot f_Y (y) \,dy$

Finalmente,

$E(X) = E(E(X |Y))$

### Exercício:

Demonstre que:

$E(Y) = E(E(Y |X))$

### Aplicações:

$E(Y|X)$ é chamado de regressão de $y$ em $x$ e é muito usado em econometria.

**Exemplo 4 (Aplicação Econometria):**

$y_i = \alpha + \beta x_i + \epsilon_i$ $E(\epsilon_i |x) = 0$

-   Calcule $E(\epsilon_i)$
-   Calcule $Cov(x_i,\epsilon_i)$

**Solução**

Usando a lei das expectativas iteradas:

$E(E(Y|X)) = E(Y)$

Aplicamos o operador esperança dos dois lados da equação $E(\epsilon_i | x) = 0$:

$E(E(\epsilon_i | x)) = E(0) = 0$

Pela LEI,

$E(\epsilon_i) = 0$

-   Calcule $Cov(x_i,\epsilon_i)$:

Podemos usar o importante corolário:

$Cov(X,Y) = E(XY) - E(X)E(Y)$

$Cov(x_i,\epsilon_i) = E(x_i \epsilon_i) - E(x_i)E(\epsilon_i)$

No item anterior encontramos $E(\epsilon_i) = 0$ portanto:

$Cov(x_i,\epsilon_i) = E(x_i \epsilon_i)$

Considere:

### Observe:

$E(XY|X) = XE(Y|X)$

$E(\sqrt{X}Y|X) = \sqrt{X}E(Y|X)$

$E(X^2 Y|X) = X^2 E(Y|X)$

### Cálculo de $E(x_i ε_i|x_i)$

$E(x_i ε_i|x_i) = x_i E(ε_i|x_i) = x_i (0) = 0$

$E(x_i ε_i|x_i) = 0$

Aplicando a LEI:

$E(E(x_i ε_i|x_i)) = E(0) = 0$

$E(x_i ε_i) = 0$

Ou seja,

$Cov(x_i,ε_i) = E(x_i ε_i) = 0$

Note que $E(x_i ε_i|x_i) = x_i E(ε_i|x_i)$ pois ao condicionarmos em $x_i$ a variável passa a ser considerada uma constante e pode "sair" da esperança.

Aplicando o operador esperança dos dois lados e usando a lei das expectativas iteradas:

$E(E(x_i ε_i|x_i)) = E(0) \rightarrow E(x_i ε_i) = 0$

Portanto $Cov(x_i,ε_i) = E(x_i∙ε_i) = 0$

### Teorema - Função Geradora de Momentos para Combinações Lineares

Sejam $X$ e $Y$ variáveis aleatórias independentes com funções geradoras de momentos $M_X (t)$ e $M_Y (t)$ respectivamente. Então a fgm da variável $Z=X+Y$ é dada por:

$M_Z (t) = M_X (t) M_Y (t)$

#### Prova:

Partindo da definição de função geradora de momentos:

$M_Z (t) = E(e^tZ) = E(e^{t(X+Y)}) = E(e^tX∙e^tY)$

$E(e^tX)∙E(e^tY) = M_X (t) M_Y (t)$

### Covariância

É o momento conjunto mais importante. A covariância entre $X$ e $Y$ é definida como:

$\sigma_{XY} = cov(X,Y) = E[(X-μ_X)(Y-μ_Y)]$

Onde, $-∞ < \sigma_{XY} < ∞$, $μ_X = E(X)$, e $μ_Y = E(Y)$.

#### Cálculo da covariância

$Cov(X,Y) = ∑_x ∑_y (x-E(X))(y-E(Y))P(X=x_i,Y=y_i)$

Para $X$ e $Y$ discretas:

$Cov(X,Y) = ∫_{-∞}^{∞} ∫_{-∞}^{∞} (x-E(X))(y-E(Y))f(x,y)dxdy$

Para $X$ e $Y$ contínuas.

Em que $P(X=x_i,Y=y_i) = f(x_i,y_i)$ é a função de probabilidade conjunta de $X$ e $Y$, e $f(x,y)$ é a função de densidade conjunta das variáveis aleatórias $X$ e $Y$.

### Correlação

Mede o grau de associação linear entre duas variáveis

$ρ_{XY} = corr(X,Y) = \frac{cov(X,Y)}{\sqrt{Var(X)} \sqrt{Var(Y)}} = \frac{cov(X,Y)}{σ_X σ_Y}$

Onde, $-1≤ρ_{XY}≤1$

Observação: Se $cov(X,Y)=0$ → $corr(X,Y)=0$, → $X$ e $Y$ não são correlacionados, ou seja, $X$ e $Y$ não são independentes

### Teorema

Se $X$ e $Y$ são independentes, então, $cov(X,Y)=corr(X,Y)=0$.

Demonstração: Usamos a condição: se $X$ e $Y$ são independentes, então:

$E(XY) = E(X)E(Y)$

Logo, da definição de covariância:

$Cov(X,Y) = E(XY) - E(X)E(Y) = E(X)E(Y) - E(X)E(Y) = 0$

Ou a forma mais geral para duas funções $U$ e $V$:

$E(U(X)V(Y)) = E(U(X))E(V(Y))$

Logo na definição de Covariância:

$\sigma_{XY} = cov(X,Y) = E[(X-μ_X)(Y-μ_Y)] = [E_{xy}(X-μ_X)][E_{xy}(Y-μ_Y)] = [E_{xy}(X)-E_{xy}(μ_X)][E_{xy}(Y)-E_{xy}(μ_Y)] = [μ_X-μ_X][μ_Y-μ_Y] = 0$

$\sigma_{XY} = cov(X,Y) = E_x(X-μ_X) E_y(Y-μ_Y) = 0$

Para a correlação:

$ρ_{XY} = \frac{cov(X,Y)}{\sqrt{Var(X)} \sqrt{Var(Y)}} = \frac{cov(X,Y)}{σ_X σ_Y} = 0$
